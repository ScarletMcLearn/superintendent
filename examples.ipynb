{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# superintendent\n",
    "\n",
    "![](logo.png)\n",
    "\n",
    "want to semi-supervise your machine learning?\n",
    "\n",
    "This package is designed to provide a `ipywidget`-based interactive labelling tool for your data.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```\n",
    "pip install superintendent\n",
    "```\n",
    "\n",
    "If you want to also use the keyboard shortcuts for labelling faster, you will\n",
    "also have to enable the ipyevents jupyter extension:\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix ipyevents\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Labelling individual data points\n",
    "\n",
    "Let's assume we have a text dataset that contains some labelled sentences and some unlabelled sentences. For example, we could get the headlines for a bunch of UK news websites (the code for this comes from the amazing github project [compare-headlines](https://github.com/isobelweinberg/compare-headlines/blob/master/scrape-headlines.ipynb) by [isobelweinberg](https://github.com/isobelweinberg)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "headlines = []\n",
    "labels = []\n",
    "\n",
    "r = requests.get('https://www.theguardian.com/uk').text #get html\n",
    "soup = BeautifulSoup(r, 'html5lib') #run html through beautiful soup\n",
    "headlines += [headline.text for headline in\n",
    "              soup.find_all('span', class_='js-headline-text')][:10]\n",
    "labels += ['guardian'] * (len(headlines) - len(labels))\n",
    "\n",
    "soup = BeautifulSoup(requests.get('http://www.dailymail.co.uk/home/index.html').text, 'html5lib')\n",
    "headlines += [headline.text.replace('\\n', '').replace('\\xa0', '').strip()\n",
    "              for headline in soup.find_all(class_=\"linkro-darkred\")][:10]\n",
    "labels += ['daily mail'] * (len(headlines) - len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume that instead of wanting to know about the source of the article, we actually want to know about how professional the headline is. But we don't have labels for the two! We can use `superintendent` to start creating some. To make sure it's nice and easy on the eyes, we'll also use a custom display function to make the text readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superintendent.semisupervisor import SemiSupervisor\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "labelling_widget = SemiSupervisor(headlines, labels = [None] * len(headlines),\n",
    "                                  display_func=lambda txt, n_samples: display.display(display.HTML(txt[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget.annotate(options=['professional', 'not professional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget.new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Labelling clusters\n",
    "\n",
    "Another common task is labelling clusters of points. Let's say, for example, that we've k-means-clustered the above data and assigned one of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superintendent.clustersupervisor import ClusterSupervisor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget = ClusterSupervisor(headlines, np.random.choice([1, 2, 3], size=len(headlines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labelling_widget.annotate(chunk_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can get the labels from the object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget.new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the cluster index -> cluster labels mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labelling_widget.new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, often when we label text clusters, we probably want to not look at all the text individually, but instead want to look at a wordcloud. We can do this by passing a word-cloud generating function to our labeller. We'll use one from the [word_cloud](https://github.com/amueller/word_cloud) package. We'll need to write a little wrapper around it to actually display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import IPython.display\n",
    "\n",
    "def show_wordcloud(text, n_samples=None):\n",
    "    text = ' '.join(text.ravel())\n",
    "    IPython.display.display(\n",
    "        WordCloud().generate(text).to_image()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget = ClusterSupervisor(\n",
    "    headlines, np.random.choice([1, 2, 3], size=len(headlines)),\n",
    "    display_func = show_wordcloud\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want the wordcloud to be drawn for the entire data set, we need to modify the chunk_size argument for our `annotate` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget.annotate(chunk_size=np.inf, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_widget.new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 3: labelling images\n",
    "\n",
    "For labelling images, there is a special factory method that sets the right display functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from superintendent.semisupervisor import SemiSupervisor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = SemiSupervisor.from_images(digits[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.annotate(options=list(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 3: labelling images\n",
    "\n",
    "The same can be done for clustered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from superintendent.clustersupervisor import ClusterSupervisor\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = TSNE(\n",
    "    metric='correlation'\n",
    ").fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans(n_clusters=10, n_jobs=-1).fit_predict(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labeller = ClusterSupervisor.from_images(digits.data, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labeller.annotate(chunk_size=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've done that, you can check how our clustering worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(digits.target == cluster_labeller.cluster_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
